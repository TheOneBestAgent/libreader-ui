# Vast.ai Bark TTS - Optimized for ghcr.io
# Models downloaded at runtime to persistent storage (not baked into image)
# This keeps the image small (~2GB) and avoids Docker Hub rate limits

FROM nvidia/cuda:12.1.1-runtime-ubuntu22.04

LABEL org.opencontainers.image.source=https://github.com/darvondoom/libread-ereader
LABEL org.opencontainers.image.description="Bark TTS Worker for Vast.ai"
LABEL org.opencontainers.image.licenses=MIT

ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1

# Install system dependencies (minimal)
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.11 \
    python3.11-venv \
    python3.11-dev \
    python3-pip \
    libsndfile1 \
    ffmpeg \
    curl \
    git \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

WORKDIR /app

# Create venv
RUN python3.11 -m venv /app/venv
ENV PATH="/app/venv/bin:$PATH"
ENV VIRTUAL_ENV="/app/venv"

# Upgrade pip
RUN pip install --no-cache-dir --upgrade pip setuptools wheel

# Install PyTorch with CUDA support (this is the bulk of the image)
RUN pip install --no-cache-dir \
    torch==2.1.2 \
    torchaudio==2.1.2 \
    --index-url https://download.pytorch.org/whl/cu121

# Install Bark and other dependencies
# Note: We install bark but DON'T download models here
RUN pip install --no-cache-dir \
    git+https://github.com/suno-ai/bark.git \
    soundfile \
    scipy \
    numpy \
    requests

# Copy worker script
COPY bark-worker.py /app/worker.py

# Create startup script that handles model download
COPY <<'EOF' /app/start.sh
#!/bin/bash
set -e

# Use Vast.ai's persistent workspace for models if available
if [ -d "/workspace" ]; then
    export HF_HOME="/workspace/models"
    export XDG_CACHE_HOME="/workspace/models"
    export TRANSFORMERS_CACHE="/workspace/models"
    mkdir -p /workspace/models
    echo "Using persistent storage at /workspace/models"
else
    export HF_HOME="/app/models"
    export XDG_CACHE_HOME="/app/models"
    export TRANSFORMERS_CACHE="/app/models"
    mkdir -p /app/models
    echo "Using local storage at /app/models"
fi

# Check if models exist
if [ -d "$HF_HOME/hub/models--suno--bark" ]; then
    echo "Bark models found in cache"
else
    echo "Downloading Bark models (first run only, ~5GB)..."
    echo "This may take 5-10 minutes..."
fi

# Start the worker
exec python /app/worker.py
EOF

RUN chmod +x /app/start.sh

# Expose port
EXPOSE 8080

# Health check (longer start period for model download)
HEALTHCHECK --interval=30s --timeout=10s --start-period=600s --retries=3 \
    CMD curl -f http://localhost:8080/health || exit 1

# Start via script
CMD ["/app/start.sh"]
